[1242] 多线程网页爬虫
=====================

-  https://leetcode-cn.com/problems/web-crawler-multithreaded

题目描述
--------

.. raw:: html

   <p>

给你一个初始地址 startUrl 和一个 HTML
解析器接口 HtmlParser，请你实现一个 多线程的网页爬虫，用于获取与 startUrl 有 相同主机名 的所有链接。 

.. raw:: html

   </p>

.. raw:: html

   <p>

以 任意 顺序返回爬虫获取的路径。

.. raw:: html

   </p>

.. raw:: html

   <p>

爬虫应该遵循：

.. raw:: html

   </p>

.. raw:: html

   <ul>

::

    <li>从&nbsp;<code>startUrl</code>&nbsp;开始</li>
    <li>调用&nbsp;<code>HtmlParser.getUrls(url)</code> 从指定网页路径获得的所有路径。</li>
    <li>不要抓取相同的链接两次。</li>
    <li>仅浏览与&nbsp;<code>startUrl</code>&nbsp;<strong>相同主机名&nbsp;</strong>的链接。</li>

.. raw:: html

   </ul>

.. raw:: html

   <p>

.. raw:: html

   </p>

.. raw:: html

   <p>

如上图所示，主机名是 example.org 。简单起见，你可以假设所有链接都采用 http
协议，并且没有指定 端口号。举个例子，链接 http://leetcode.com/problems
和链接 http://leetcode.com/contest 属于同一个 主机名，
而 http://example.org/test 与 http://example.com/abc
并不属于同一个 主机名。

.. raw:: html

   </p>

.. raw:: html

   <p>

HtmlParser 的接口定义如下：

.. raw:: html

   </p>

.. raw:: html

   <pre>
   interface HtmlParser {
     // Return a list of all urls from a webpage of given <em>url</em>.
     // This is a blocking call, that means it will do HTTP request and return when this request is finished.
     public List&lt;String&gt; getUrls(String url);
   }</pre>

.. raw:: html

   <p>

注意一点，getUrls(String url) 模拟执行一个HTTP的请求。
你可以将它当做一个阻塞式的方法，直到请求结束。 getUrls(String
url) 保证会在 15ms 内返回所有的路径。
单线程的方案会超过时间限制，你能用多线程方案做的更好吗？

.. raw:: html

   </p>

.. raw:: html

   <p>

对于问题所需的功能，下面提供了两个例子。为了方便自定义测试，你可以声明三个变量 urls，edges 和 startUrl。但要注意你只能在代码中访问 startUrl，并不能直接访问 urls 和 edges。

.. raw:: html

   </p>

.. raw:: html

   <p>

 

.. raw:: html

   </p>

.. raw:: html

   <p>

拓展问题：

.. raw:: html

   </p>

.. raw:: html

   <ol>

::

    <li>假设我们要要抓取 10000 个节点和 10 亿个路径。并且在每个节点部署相同的的软件。软件可以发现所有的节点。我们必须尽可能减少机器之间的通讯，并确保每个节点负载均衡。你将如何设计这个网页爬虫？</li>
    <li>如果有一个节点发生故障不工作该怎么办？</li>
    <li>如何确认爬虫任务已经完成？</li>

.. raw:: html

   </ol>

.. raw:: html

   <p>

 

.. raw:: html

   </p>

.. raw:: html

   <p>

示例 1：

.. raw:: html

   </p>

.. raw:: html

   <p>

.. raw:: html

   </p>

.. raw:: html

   <pre>
   <strong>输入：
   </strong>urls = [
   &nbsp; &quot;http://news.yahoo.com&quot;,
   &nbsp; &quot;http://news.yahoo.com/news&quot;,
   &nbsp; &quot;http://news.yahoo.com/news/topics/&quot;,
   &nbsp; &quot;http://news.google.com&quot;,
   &nbsp; &quot;http://news.yahoo.com/us&quot;
   ]
   edges = [[2,0],[2,1],[3,2],[3,1],[0,4]]
   startUrl = &quot;http://news.yahoo.com/news/topics/&quot;
   <strong>输出：</strong>[
   &nbsp; &quot;http://news.yahoo.com&quot;,
   &nbsp; &quot;http://news.yahoo.com/news&quot;,
   &nbsp; &quot;http://news.yahoo.com/news/topics/&quot;,
   &nbsp; &quot;http://news.yahoo.com/us&quot;
   ]
   </pre>

.. raw:: html

   <p>

示例 2：

.. raw:: html

   </p>

.. raw:: html

   <p>

.. raw:: html

   </p>

.. raw:: html

   <pre>
   <strong>输入：</strong>
   urls = [
   &nbsp; &quot;http://news.yahoo.com&quot;,
   &nbsp; &quot;http://news.yahoo.com/news&quot;,
   &nbsp; &quot;http://news.yahoo.com/news/topics/&quot;,
   &nbsp; &quot;http://news.google.com&quot;
   ]
   edges = [[0,2],[2,1],[3,2],[3,1],[3,0]]
   startUrl = &quot;http://news.google.com&quot;
   <strong>输出：</strong>[&quot;http://news.google.com&quot;]
   <strong>解释：</strong>startUrl 链接与其他页面不共享一个主机名。</pre>

.. raw:: html

   <p>

 

.. raw:: html

   </p>

.. raw:: html

   <p>

提示：

.. raw:: html

   </p>

.. raw:: html

   <ul>

::

    <li><code>1 &lt;= urls.length &lt;= 1000</code></li>
    <li><code>1 &lt;= urls[i].length &lt;= 300</code></li>
    <li><code>startUrl</code>&nbsp;是&nbsp;<code>urls</code>&nbsp;中的一个。</li>
    <li>主机名的长度必须为 1 到 63 个字符（包括点 <code>.</code> 在内），只能包含从 &ldquo;a&rdquo; 到 &ldquo;z&rdquo; 的 ASCII 字母和 &ldquo;0&rdquo; 到 &ldquo;9&rdquo; 的数字，以及中划线 &ldquo;-&rdquo;。</li>
    <li>主机名开头和结尾不能是中划线 &ldquo;-&rdquo;。</li>
    <li>参考资料：<a href="https://en.wikipedia.org/wiki/Hostname#Restrictions_on_valid_hostnames">https://en.wikipedia.org/wiki/Hostname#Restrictions_on_valid_hostnames</a></li>
    <li>你可以假设路径都是不重复的。</li>

.. raw:: html

   </ul>

.. raw:: html

   <div>

.. raw:: html

   <div>

Related Topics

.. raw:: html

   </div>

.. raw:: html

   <div>

.. raw:: html

   <li>

深度优先搜索

.. raw:: html

   </li>

.. raw:: html

   <li>

广度优先搜索

.. raw:: html

   </li>

.. raw:: html

   </div>

.. raw:: html

   </div>

题目代码
--------

.. code:: cpp

    /**
     * // This is the HtmlParser's API interface.
     * // You should not implement it, or speculate about its implementation
     * class HtmlParser {
     *   public:
     *     vector<string> getUrls(string url);
     * };
     */
    class Solution {
    public:
        vector<string> crawl(string startUrl, HtmlParser htmlParser) {
            
        }
    };

题目解析
--------

方法一
~~~~~~

分析
^^^^

思路
^^^^

注意
^^^^

知识点
^^^^^^

复杂度
^^^^^^

参考
^^^^

答案
^^^^

.. code:: cpp

    //

方法二
~~~~~~

分析
^^^^

思路
^^^^

注意
^^^^

知识点
^^^^^^

复杂度
^^^^^^

参考
^^^^

答案
^^^^

.. code:: cpp

    //
